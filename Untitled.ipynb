{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41229849",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'shap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01munder_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomUnderSampler\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrappers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscikit_learn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasClassifier\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshap\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mskimage\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01meli5\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'shap'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Concatenate, Flatten\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, BorderlineSMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "import shap\n",
    "import skimage\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from eli5.permutation_importance import get_score_importances\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from mlxtend.plotting import plot_learning_curves\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from itertools import product\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "from sklearn.utils import shuffle\n",
    "from matplotlib import style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74a6b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "style.use('dark_background')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b6bd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기 및 필요한 column만 추출\n",
    "\n",
    "viral_uveitis_data = pd.read_csv('/Users/harock96/Documents/project/연대 인턴/data/2005-2022 viral uveitis_220624_revised.csv')\n",
    "\n",
    "core_cols_list = ['연구등록번호', 'Diagnosis','Gender', '진단시점나이', 'CMV IgM[Serum]', 'CMV IgG[Serum]',\n",
    "                    'HSV IgM[Serum]','HSV IgG[Serum]', 'VZV IgM[Serum]','VZV IgG[Serum]',\n",
    "                    'WBC COUNT[Whole blood]', 'Lymphocyte(#)[Whole blood]','Lymphocyte(%)[Whole blood]',\n",
    "                    'Monocyte(#)[Whole blood]', 'Monocyte(%)[Whole blood]','Neutrophil(#)[Whole blood]',\n",
    "                    'Neutrophil(%)[Whole blood]', 'ESR[Whole blood]', 'CRP[Serum]']\n",
    "\n",
    "\n",
    "\n",
    "viral_uveitis_data = viral_uveitis_data[core_cols_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbd0681",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 연구등록번호가 여러 개일때 이전의 검사결과가 nan 이라면 다음의 검사결과로 채움\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "\n",
    "while (i < 9999):\n",
    "\n",
    "    while (viral_uveitis_data.loc[i]['연구등록번호'] == viral_uveitis_data.loc[i+j+1]['연구등록번호']):\n",
    "\n",
    "        if str(viral_uveitis_data.loc[i]['CMV IgM[Serum]']) == 'nan':\n",
    "            viral_uveitis_data.loc[i, 'CMV IgM[Serum]'] = viral_uveitis_data.loc[i+j+1]['CMV IgM[Serum]']\n",
    "            \n",
    "        if str(viral_uveitis_data.loc[i]['CMV IgG[Serum]']) == 'nan':\n",
    "            viral_uveitis_data.loc[i, 'CMV IgG[Serum]'] = viral_uveitis_data.loc[i+j+1]['CMV IgG[Serum]']\n",
    "                \n",
    "        if str(viral_uveitis_data.loc[i]['HSV IgM[Serum]']) == 'nan':\n",
    "             viral_uveitis_data.loc[i, 'HSV IgM[Serum]'] = viral_uveitis_data.loc[i+j+1]['HSV IgM[Serum]']\n",
    "                \n",
    "        if str(viral_uveitis_data.loc[i]['HSV IgG[Serum]']) == 'nan':\n",
    "             viral_uveitis_data.loc[i, 'HSV IgG[Serum]'] = viral_uveitis_data.loc[i+j+1]['HSV IgG[Serum]']\n",
    "                \n",
    "        if str(viral_uveitis_data.loc[i]['VZV IgM[Serum]']) == 'nan':\n",
    "             viral_uveitis_data.loc[i, 'VZV IgM[Serum]'] = viral_uveitis_data.loc[i+j+1]['VZV IgM[Serum]']\n",
    "                \n",
    "        if str(viral_uveitis_data.loc[i]['VZV IgG[Serum]']) == 'nan':\n",
    "            viral_uveitis_data.loc[i, 'VZV IgG[Serum]'] = viral_uveitis_data.loc[i+j+1]['VZV IgG[Serum]']\n",
    "                \n",
    "        if str(viral_uveitis_data.loc[i]['WBC COUNT[Whole blood]']) == 'nan':\n",
    "            viral_uveitis_data.loc[i, 'WBC COUNT[Whole blood]'] = viral_uveitis_data.loc[i+j+1]['WBC COUNT[Whole blood]']\n",
    "                \n",
    "        if str(viral_uveitis_data.loc[i]['Lymphocyte(#)[Whole blood]']) == 'nan':\n",
    "             viral_uveitis_data.loc[i, 'Lymphocyte(#)[Whole blood]'] = viral_uveitis_data.loc[i+j+1]['Lymphocyte(#)[Whole blood]']\n",
    "                \n",
    "        if str(viral_uveitis_data.loc[i]['Lymphocyte(%)[Whole blood]']) == 'nan':\n",
    "             viral_uveitis_data.loc[i, 'Lymphocyte(%)[Whole blood]'] = viral_uveitis_data.loc[i+j+1]['Lymphocyte(%)[Whole blood]']\n",
    "                \n",
    "        if str(viral_uveitis_data.loc[i]['Monocyte(#)[Whole blood]']) == 'nan':\n",
    "            viral_uveitis_data.loc[i, 'Monocyte(#)[Whole blood]'] = viral_uveitis_data.loc[i+j+1]['Monocyte(#)[Whole blood]']\n",
    "                \n",
    "        if str(viral_uveitis_data.loc[i]['Monocyte(%)[Whole blood]']) == 'nan':\n",
    "            viral_uveitis_data.loc[i]['Monocyte(%)[Whole blood]'] = viral_uveitis_data.loc[i+j+1]['Monocyte(%)[Whole blood]']\n",
    "                \n",
    "        if str(viral_uveitis_data.loc[i]['Neutrophil(#)[Whole blood]']) == 'nan':\n",
    "            viral_uveitis_data.loc[i]['Neutrophil(#)[Whole blood]'] = viral_uveitis_data.loc[i+j+1]['Neutrophil(#)[Whole blood]']\n",
    "                \n",
    "        if str(viral_uveitis_data.loc[i]['Neutrophil(%)[Whole blood]']) == 'nan':\n",
    "            viral_uveitis_data.loc[i, 'Neutrophil(%)[Whole blood]'] = viral_uveitis_data.loc[i+j+1]['Neutrophil(%)[Whole blood]']\n",
    "                \n",
    "        if str(viral_uveitis_data.loc[i]['ESR[Whole blood]']) == 'nan':\n",
    "            viral_uveitis_data.loc[i, 'ESR[Whole blood]'] = viral_uveitis_data.loc[i+j+1]['ESR[Whole blood]']\n",
    "            \n",
    "        if str(viral_uveitis_data.loc[i]['CRP[Serum]']) == 'nan':\n",
    "            viral_uveitis_data.loc[i, 'CRP[Serum]'] = viral_uveitis_data.loc[i+j+1]['CRP[Serum]']\n",
    "\n",
    "        j = j+1\n",
    "                     \n",
    "    i = i+j+1\n",
    "    j = 0\n",
    "    \n",
    "    print(f'i=={i}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778a8bfa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 데이터 전처리\n",
    "\n",
    "vu_data_rev = viral_uveitis_data.drop_duplicates(subset = '연구등록번호', keep = 'first') # 중복된 row 제거(0,1),(0,2) 일 때 1이나 2 선택\n",
    "\n",
    "cols_to_norm = ['진단시점나이', 'CMV IgM[Serum]', 'CMV IgG[Serum]',\n",
    "                    'HSV IgM[Serum]','HSV IgG[Serum]', 'VZV IgM[Serum]','VZV IgG[Serum]',\n",
    "                    'WBC COUNT[Whole blood]', 'Lymphocyte(#)[Whole blood]','Lymphocyte(%)[Whole blood]',\n",
    "                    'Monocyte(#)[Whole blood]', 'Monocyte(%)[Whole blood]','Neutrophil(#)[Whole blood]',\n",
    "                    'Neutrophil(%)[Whole blood]', 'ESR[Whole blood]', 'CRP[Serum]']\n",
    "\n",
    "vu_data_rev[cols_to_norm] = vu_data_rev[cols_to_norm].astype(float) # 결측치 처리 위해 type 변환\n",
    "vu_data_rev[cols_to_norm] = StandardScaler().fit_transform(vu_data_rev[cols_to_norm]) # 데이터 표준화\n",
    "\n",
    "# class 별로 분류\n",
    "\n",
    "vu_data_rev_0 = vu_data_rev[vu_data_rev.Diagnosis == 0]\n",
    "\n",
    "vu_data_rev_1 = vu_data_rev[vu_data_rev.Diagnosis == 1]\n",
    "\n",
    "vu_data_rev_2 = vu_data_rev[vu_data_rev.Diagnosis == 2]\n",
    "\n",
    "# class 별로 결측치 대체\n",
    "\n",
    "vu_data_rev_0 = vu_data_rev_0.replace(np.NaN, vu_data_rev_0.mean())\n",
    "\n",
    "vu_data_rev_1 = vu_data_rev_1.replace(np.NaN, vu_data_rev_1.mean())\n",
    "\n",
    "vu_data_rev_2 = vu_data_rev_2.replace(np.NaN, vu_data_rev_2.mean())\n",
    "\n",
    "\n",
    "# 데이터 concat\n",
    "\n",
    "input_cols_lst = ['Gender', '진단시점나이', 'CMV IgM[Serum]', 'CMV IgG[Serum]', 'HSV IgM[Serum]','HSV IgG[Serum]',\n",
    "                  'VZV IgM[Serum]','VZV IgG[Serum]', 'WBC COUNT[Whole blood]', 'Lymphocyte(#)[Whole blood]',\n",
    "                  'Lymphocyte(%)[Whole blood]','Monocyte(#)[Whole blood]', 'Monocyte(%)[Whole blood]',\n",
    "                  'Neutrophil(#)[Whole blood]','Neutrophil(%)[Whole blood]', 'ESR[Whole blood]', 'CRP[Serum]']\n",
    "\n",
    "vu_data_rev_01 = pd.concat([vu_data_rev_0, vu_data_rev_1])\n",
    "vu_data_rev_02 = pd.concat([vu_data_rev_0, vu_data_rev_2])\n",
    "\n",
    "vu_data_rev_02['Diagnosis'] = vu_data_rev_02['Diagnosis'].replace(2, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e68c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 split \n",
    "\n",
    "input_cols_lst1 = ['진단시점나이', 'CMV IgG[Serum]', 'HSV IgG[Serum]',\n",
    "                  'VZV IgM[Serum]','VZV IgG[Serum]', 'WBC COUNT[Whole blood]', \n",
    "                  'Lymphocyte(%)[Whole blood]','Monocyte(#)[Whole blood]', 'Monocyte(%)[Whole blood]',\n",
    "                  'Neutrophil(#)[Whole blood]','Neutrophil(%)[Whole blood]',  'CRP[Serum]']\n",
    "                  \n",
    "\n",
    "\n",
    "data = vu_data_rev_01[input_cols_lst]\n",
    "target = vu_data_rev_01['Diagnosis']\n",
    "\n",
    "data = data.astype(np.float32)\n",
    "target = target.astype(np.float32)\n",
    "\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(data, target, \n",
    "                                                    test_size=0.4, stratify = target,\n",
    "                                                    shuffle = True, random_state = 42)\n",
    "\n",
    "target = y_test1\n",
    "\n",
    "X_test1, X_val1, y_test1, y_val1 = train_test_split(X_test1, y_test1, \n",
    "                                                    test_size= 0.125, stratify = target,\n",
    "                                                    shuffle = True,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b5254e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01 모델 train\n",
    "\n",
    "# 데이터 split \n",
    "\n",
    "input_cols_lst1 = ['진단시점나이', 'CMV IgG[Serum]', 'HSV IgG[Serum]',\n",
    "                  'VZV IgM[Serum]','VZV IgG[Serum]', 'WBC COUNT[Whole blood]', \n",
    "                  'Lymphocyte(%)[Whole blood]','Monocyte(#)[Whole blood]', 'Monocyte(%)[Whole blood]',\n",
    "                  'Neutrophil(#)[Whole blood]','Neutrophil(%)[Whole blood]',  'CRP[Serum]']\n",
    "                  \n",
    "\n",
    "\n",
    "data = vu_data_rev_01[input_cols_lst]\n",
    "target = vu_data_rev_01['Diagnosis']\n",
    "\n",
    "data = data.astype(np.float32)\n",
    "target = target.astype(np.float32)\n",
    "\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(data, target, \n",
    "                                                    test_size=0.4, stratify = target,\n",
    "                                                    shuffle = True, random_state = 42)\n",
    "\n",
    "target = y_test1\n",
    "\n",
    "X_test1, X_val1, y_test1, y_val1 = train_test_split(X_test1, y_test1, \n",
    "                                                    test_size= 0.125, stratify = target,\n",
    "                                                    shuffle = True,random_state = 42)\n",
    "\n",
    "# class weight\n",
    "\n",
    "class_weight1 = class_weight.compute_class_weight(\"balanced\", classes = np.unique(y_train1), y = y_train1)[::-1]\n",
    "class_weight1 = dict(enumerate(class_weight1.flatten(), 0))\n",
    "\n",
    "# auc history clear\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# model 구현\n",
    "\n",
    "mlp1 = keras.Sequential()\n",
    "mlp1.add(keras.layers.Dense(100, activation = 'sigmoid', input_shape = (len(X_train1.columns.tolist()),)))\n",
    "mlp1.add(keras.layers.Dense(1,activation = 'sigmoid'))\n",
    "mlp1.compile(optimizer = 'adam', loss ='binary_crossentropy', metrics = tf.keras.metrics.AUC())\n",
    "mlp1.summary()\n",
    "\n",
    "mlp1_model = mlp1.fit(X_train1, y_train1, epochs = 2000, batch_size = 256, class_weight = class_weight1,\n",
    "                      validation_data = (X_val1, y_val1), verbose = 2)\n",
    "mlp1.evaluate(X_test1,y_test1)\n",
    "\n",
    "# auc, roc_curve\n",
    "\n",
    "y_pred = mlp1.predict(X_test1).ravel()\n",
    "\n",
    "y = y_test1.to_numpy()\n",
    " \n",
    "mlp_fpr1, mlp_tpr1, mlp_thresholds1 = roc_curve(y, y_pred, pos_label = 1)\n",
    "\n",
    "print(\"MLP task1 auc :\", auc(mlp_fpr1,mlp_tpr1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0999b4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.title(\"MLP task1 auc Curve\")\n",
    "plt.plot(mlp1_model.history['val_auc'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('auc')\n",
    "plt.legend(['val'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ef13fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"MLP task1 loss Curve\")\n",
    "plt.plot(mlp1_model.history['val_loss'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['val'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2ed637",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"MLP task1 ROC Curve\")\n",
    "plt.plot(mlp_fpr1, mlp_tpr1)\n",
    "plt.plot([0,1], [0,1], '--')\n",
    "plt.ylabel(\"sensitivity\")\n",
    "plt.xlabel(\"1-specificity\")\n",
    "plt.legend(['ROC'])\n",
    "round(auc(mlp_fpr1,mlp_tpr1),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eee2ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def mlp1_model():\n",
    "    mlp1 = keras.Sequential()\n",
    "    mlp1.add(keras.layers.Dense(100, activation = 'sigmoid', input_shape = (len(X_train1.columns.tolist()),)))\n",
    "    mlp1.add(keras.layers.Dense(1,activation = 'sigmoid'))\n",
    "    mlp1.compile(optimizer = 'adam', loss ='binary_crossentropy', metrics = tf.keras.metrics.AUC())\n",
    "    return mlp1 \n",
    "\n",
    "model1 = KerasClassifier(build_fn= mlp1_model, epochs = 1500, batch_size = 256, class_weight = class_weight1, verbose =2)    \n",
    "model1.fit(X_train1,y_train1)\n",
    "\n",
    "perm = PermutationImportance(model1, scoring= \"roc_auc\", random_state = 42).fit(X_test1,y_test1)\n",
    "eli5.show_weights(perm, feature_names = X_train1.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4aed899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 split \n",
    "\n",
    "input_cols_lst2 = ['Gender', 'CMV IgM[Serum]', 'CMV IgG[Serum]', 'HSV IgM[Serum]','HSV IgG[Serum]',\n",
    "                  'VZV IgM[Serum]','VZV IgG[Serum]', 'WBC COUNT[Whole blood]', 'Lymphocyte(#)[Whole blood]',\n",
    "                  'Lymphocyte(%)[Whole blood]', 'Monocyte(%)[Whole blood]'\n",
    "                  ,'Neutrophil(%)[Whole blood]', 'ESR[Whole blood]', 'CRP[Serum]']\n",
    "\n",
    "data = vu_data_rev_02[input_cols_lst]\n",
    "target = vu_data_rev_02['Diagnosis']\n",
    "\n",
    "data = data.astype(np.float32)\n",
    "target = target.astype(np.float32)\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(data, target, \n",
    "                                                    test_size=0.4, stratify = target,\n",
    "                                                    shuffle = True, random_state = 42)\n",
    "\n",
    "target = y_test2\n",
    "\n",
    "X_test2, X_val2, y_test2, y_val2 = train_test_split(X_test2, target, \n",
    "                                                    test_size=0.125, stratify = target,\n",
    "                                                    shuffle = True,random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f59fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02 모델 train\n",
    "\n",
    "# class weight\n",
    "\n",
    "class_weight2 = class_weight.compute_class_weight(\"balanced\", classes = np.unique(y_train2), y = y_train2)[::-1]\n",
    "class_weight2 = dict(enumerate(class_weight2.flatten(), 0))\n",
    "\n",
    "# auc history clear\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# model 구현\n",
    "\n",
    "mlp2 = keras.Sequential()\n",
    "mlp2.add(keras.layers.Dense(100, activation = 'sigmoid', input_shape = (len(X_train2.columns.tolist()),)))\n",
    "mlp2.add(keras.layers.Dense(1,activation = 'sigmoid'))\n",
    "mlp2.compile(optimizer = 'adam', loss ='binary_crossentropy', metrics = tf.keras.metrics.AUC())\n",
    "mlp2.summary()\n",
    "\n",
    "mlp2_model = mlp2.fit(X_train2, y_train2, epochs = 500 , batch_size = 256 ,\n",
    "                      validation_data = (X_val2, y_val2), verbose = 2)\n",
    "mlp2.evaluate(X_test2,y_test2)\n",
    "\n",
    "# auc, roc_curve\n",
    "\n",
    "y_pred = mlp2.predict(X_test2).ravel()\n",
    "\n",
    "y = y_test2.to_numpy()\n",
    " \n",
    "mlp_fpr2, mlp_tpr2, mlp_thresholds2 = roc_curve(y, y_pred, pos_label = 1)\n",
    "\n",
    "auc(mlp_fpr2,mlp_tpr2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27955ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59192582",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.title(\"MLP task2 auc Curve\")\n",
    "plt.plot(mlp2_model.history['val_auc'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('auc')\n",
    "plt.legend(['val'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260a8b79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.title(\"MLP task2 loss Curve\")\n",
    "plt.plot(mlp2_model.history['val_loss'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['val'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21749a15",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.title(\"MLP task2 ROC Curve\")\n",
    "plt.plot(mlp_fpr2, mlp_tpr2)\n",
    "plt.plot([0,1], [0,1], '--')\n",
    "plt.ylabel(\"sensitivity\")\n",
    "plt.xlabel(\"1-specificity\")\n",
    "plt.legend(['ROC'])\n",
    "round(auc(mlp_fpr2,mlp_tpr2),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b34bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp2_model():\n",
    "    mlp2 = keras.Sequential()\n",
    "    mlp2.add(keras.layers.Dense(100, activation = 'sigmoid', input_shape = (len(X_train2.columns.tolist()),)))\n",
    "    mlp2.add(keras.layers.Dense(1,activation = 'sigmoid'))\n",
    "    mlp2.compile(optimizer = 'adam', loss ='binary_crossentropy', metrics = tf.keras.metrics.AUC())\n",
    "    return mlp2\n",
    "\n",
    "\n",
    "model2 = KerasClassifier(build_fn = mlp2_model, epochs = 500, batch_size = 256, verbose =2)    \n",
    "model2.fit(X_train2,y_train2)\n",
    "\n",
    "perm = PermutationImportance(model2, scoring= \"roc_auc\", random_state = 42).fit(X_test2,y_test2)\n",
    "eli5.show_weights(perm, feature_names = X_train2.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c095b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task1 데이터 split \n",
    "\n",
    "'''\n",
    "input_cols_lst1 = ['진단시점나이', 'CMV IgG[Serum]', 'HSV IgG[Serum]',\n",
    "                  'VZV IgM[Serum]','VZV IgG[Serum]', 'WBC COUNT[Whole blood]', \n",
    "                  'Lymphocyte(%)[Whole blood]','Monocyte(#)[Whole blood]', 'Monocyte(%)[Whole blood]',\n",
    "                  'Neutrophil(#)[Whole blood]','Neutrophil(%)[Whole blood]',  'CRP[Serum]']'''\n",
    "\n",
    "data = vu_data_rev_01[input_cols_lst]\n",
    "target = vu_data_rev_01['Diagnosis']\n",
    "\n",
    "data = data.astype(np.float32)\n",
    "target = target.astype(np.float32)\n",
    "\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(data, target, \n",
    "                                                    test_size=0.4, stratify = target,\n",
    "                                                    shuffle = True, random_state = 42)\n",
    "\n",
    "target = y_test1\n",
    "\n",
    "X_test1, X_val1, y_test1, y_val1 = train_test_split(X_test1, y_test1, \n",
    "                                                    test_size= 0.125, stratify = target,\n",
    "                                                    shuffle = True,random_state = 42)\n",
    "# task2 데이터 split \n",
    "\n",
    "'''input_cols_lst2 = ['Gender', 'CMV IgM[Serum]', 'CMV IgG[Serum]', 'HSV IgM[Serum]','HSV IgG[Serum]',\n",
    "                  'VZV IgM[Serum]','VZV IgG[Serum]', 'WBC COUNT[Whole blood]', 'Lymphocyte(#)[Whole blood]',\n",
    "                  'Lymphocyte(%)[Whole blood]', 'Monocyte(%)[Whole blood]'\n",
    "                  ,'Neutrophil(%)[Whole blood]', 'ESR[Whole blood]', 'CRP[Serum]']'''\n",
    "\n",
    "data = vu_data_rev_02[input_cols_lst]\n",
    "target = vu_data_rev_02['Diagnosis']\n",
    "\n",
    "data = data.astype(np.float32)\n",
    "target = target.astype(np.float32)\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(data, target, \n",
    "                                                    test_size=0.4, stratify = target,\n",
    "                                                    shuffle = True, random_state = 42)\n",
    "\n",
    "target = y_test2\n",
    "\n",
    "X_test2, X_val2, y_test2, y_val2 = train_test_split(X_test2, target, \n",
    "                                                    test_size=0.125, stratify = target,\n",
    "                                                    shuffle = True,random_state = 42)\n",
    "\n",
    "# input data size 맞추기\n",
    "\n",
    "X_train2_mtl = X_train2[:len(y_train1)]\n",
    "X_val2_mtl = X_val2[:len(y_val1)]\n",
    "X_test2_mtl = X_test2[:len(y_test1)]\n",
    "y_train2_mtl = y_train2[:len(y_train1)]\n",
    "y_val2_mtl = y_val2[:len(y_val1)]\n",
    "y_test2_mtl = y_test2[:len(y_test1)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16d2925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_class_weights(class_series, multi_class=True, one_hot_encoded=False):\n",
    "    \n",
    "    \n",
    "  if multi_class:\n",
    "    # If class is one hot encoded, transform to categorical labels to use compute_class_weight   \n",
    "    if one_hot_encoded:\n",
    "      class_series = np.argmax(class_series, axis=1)\n",
    "  \n",
    "    # Compute class weights with sklearn method\n",
    "    class_labels = np.unique(class_series)\n",
    "    class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=class_labels, y=class_series)\n",
    "    return dict(zip(class_labels, class_weights))\n",
    "  else:\n",
    "    # It is neccessary that the multi-label values are one-hot encoded\n",
    "    mlb = None\n",
    "    if not one_hot_encoded:\n",
    "      mlb = MultiLabelBinarizer()\n",
    "      class_series = mlb.fit_transform(class_series)\n",
    "\n",
    "    n_samples = len(class_series)\n",
    "    n_classes = len(class_series[0])\n",
    "\n",
    "    # Count each class frequency\n",
    "    class_count = [0] * n_classes\n",
    "    for classes in class_series:\n",
    "        for index in range(n_classes):\n",
    "            if classes[index] != 0:\n",
    "                class_count[index] += 1\n",
    "    \n",
    "    # Compute class weights using balanced method\n",
    "    class_weights = [n_samples / (n_classes * freq) if freq > 0 else 1 for freq in class_count]\n",
    "    class_labels = range(len(class_weights)) if mlb is None else mlb.classes_\n",
    "\n",
    "    return dict(zip(class_labels, class_weights))\n",
    "\n",
    "def weighted_binary_crossentropy(zero_weight, one_weight):\n",
    "\n",
    "    def weighted_binary_crossentropy(y_true, y_pred):\n",
    "\n",
    "        b_ce = K.binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "        # weighted calc\n",
    "        weight_vector = y_true * one_weight + (1 - y_true) * zero_weight\n",
    "        weighted_b_ce = weight_vector * b_ce\n",
    "\n",
    "        return K.mean(weighted_b_ce)\n",
    "\n",
    "    return weighted_binary_crossentropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23610a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight1 = generate_class_weights(y_train1)\n",
    "class_weight2_mtl = generate_class_weights(y_train2_mtl)\n",
    "\n",
    "\n",
    "weight1 = np.array([class_weight1[0],class_weight1[1]])\n",
    "weight2 = np.array([class_weight2_mtl[0],class_weight2_mtl[1]])\n",
    "\n",
    "loss1 = weighted_binary_crossentropy(weight1[1],weight1[0])\n",
    "loss2 = weighted_binary_crossentropy(weight2[1],weight2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbae0b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_binary_crossentropy(zero_weight, one_weight):\n",
    "\n",
    "    def weighted_binary_crossentropy(y_true, y_pred):\n",
    "\n",
    "        b_ce = K.binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "        # weighted calc\n",
    "        weight_vector = y_true * one_weight + (1 - y_true) * zero_weight\n",
    "        weighted_b_ce = weight_vector * b_ce\n",
    "\n",
    "        return K.mean(weighted_b_ce)\n",
    "\n",
    "    return weighted_binary_crossentropy\n",
    "\n",
    "weight1 = np.array([class_weight1[0],class_weight1[1]])\n",
    "weight2 = np.array([class_weight2_mtl[0],class_weight2_mtl[1]])\n",
    "\n",
    "loss1 = weighted_binary_crossentropy(weight1[1],weight1[0])\n",
    "loss2 = weighted_binary_crossentropy(weight2[1],weight2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ecb273",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# auc history clear\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Fully-shared Multi-Task Learning (MTL)\n",
    "\n",
    "input1 = Input(shape= (len(X_train1.columns.tolist()),), name = 'input1')\n",
    "x1 = Dense(100, activation = 'sigmoid')(input1)\n",
    "#x1 = Dense(50, activation = 'sigmoid')(x1)\n",
    "x1 = Model(inputs = input1, outputs = x1)\n",
    "\n",
    "input2 = Input(shape= (len(X_train2_mtl.columns.tolist()),), name = 'input2')\n",
    "x2 = Dense(100, activation = 'sigmoid')(input2)\n",
    "#x2 = Dense(50, activation = 'sigmoid')(x2)\n",
    "x2 = Model(inputs = input2, outputs = x2)\n",
    "  \n",
    "merged = Concatenate()([x1.output,x2.output])\n",
    "   \n",
    "output1 = Dense(1, activation = \"sigmoid\", name = 'output1')(merged)\n",
    "output2 = Dense(1, activation = \"sigmoid\", name = 'output2')(merged)\n",
    "\n",
    "fully_shared = Model(inputs = [x1.input, x2.input],  outputs = [output1, output2])\n",
    "\n",
    "fully_shared.compile(loss = {'output1' : loss1, 'output2' : loss2}, optimizer = 'adam', metrics = tf.keras.metrics.AUC())\n",
    "fully_shared.summary()\n",
    "\n",
    "fully_shared_model = fully_shared.fit([X_train1,X_train2_mtl], \n",
    "                 [y_train1,y_train2_mtl],\n",
    "                 epochs = 1000, batch_size = 256 , \n",
    "                 validation_data = ([X_val1,X_val2_mtl],\n",
    "                                    [y_val1,y_val2_mtl]), \n",
    "                 verbose = 2) \n",
    "\n",
    "\n",
    "y_pred = fully_shared.predict([X_test1,X_test2_mtl])[0].ravel()\n",
    "\n",
    "y = y_test1.to_numpy()\n",
    " \n",
    "mtl_fpr1, mtl_tpr1, thresholds1 = roc_curve(y, y_pred, pos_label = 1)\n",
    "\n",
    "print(\"fully-shared MTL task1 auc :\", auc(mtl_fpr1,mtl_tpr1))\n",
    "\n",
    "y_pred = fully_shared.predict([X_test1,X_test2_mtl])[1].ravel()\n",
    "\n",
    "y = y_test2_mtl.to_numpy()\n",
    " \n",
    "mtl_fpr2, mtl_tpr2, thresholds2 = roc_curve(y, y_pred, pos_label = 1)\n",
    "\n",
    "print(\"fully-shared MTL task2 auc :\", auc(mtl_fpr2,mtl_tpr2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26ed725",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"fully-shared MTL task2 loss\")\n",
    "plt.plot(fully_shared_model.history['val_output1_loss'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['val'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec25551",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"fully-shared MTL loss\")\n",
    "plt.plot(fully_shared_model.history['val_loss'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['val'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0699b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"fully-shared MTL task1 ROC curve\")\n",
    "plt.plot(mtl_fpr1, mtl_tpr1)\n",
    "plt.plot([0,1], [0,1], '--')\n",
    "plt.ylabel(\"sensitivity\")\n",
    "plt.xlabel(\"1-specificity\")\n",
    "plt.legend(['ROC'])\n",
    "round(auc(mtl_fpr1,mtl_tpr1),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf02b7a0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.title(\"fully-shared MTL task2 ROC curve\")\n",
    "plt.plot(mtl_fpr2, mtl_tpr2)\n",
    "plt.plot([0,1], [0,1], '--')\n",
    "plt.ylabel(\"sensitivity\")\n",
    "plt.xlabel(\"1-specificity\")\n",
    "plt.legend(['ROC'])\n",
    "round(auc(mtl_fpr2,mtl_tpr2),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5b283a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight1 = generate_class_weights(y_train1)\n",
    "class_weight2_mtl = generate_class_weights(y_train2_mtl)\n",
    "\n",
    "\n",
    "weight1 = np.array([class_weight1[0],class_weight1[1]])\n",
    "weight2 = np.array([class_weight2_mtl[0],class_weight2_mtl[1]])\n",
    "\n",
    "loss1 = weighted_binary_crossentropy(weight1[1],weight1[0])\n",
    "loss2 = weighted_binary_crossentropy(weight2[1],weight2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a15ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared-Private MTL 데이터\n",
    "\n",
    "shared_cols_lst = ['CMV IgG[Serum]', 'HSV IgG[Serum]',\n",
    "                  'VZV IgM[Serum]','VZV IgG[Serum]', 'WBC COUNT[Whole blood]', \n",
    "                  'Lymphocyte(%)[Whole blood]', 'Monocyte(%)[Whole blood]',\n",
    "                  'Neutrophil(%)[Whole blood]', 'CRP[Serum]']\n",
    "\n",
    "task1_private_cols_lst = ['진단시점나이', 'Neutrophil(#)[Whole blood]', 'Monocyte(#)[Whole blood]']\n",
    "task2_private_cols_lst = ['Gender', 'CMV IgM[Serum]', 'HSV IgM[Serum]',  'Lymphocyte(#)[Whole blood]', 'ESR[Whole blood]']\n",
    "\n",
    "X_train1_private = X_train1[task1_private_cols_lst]\n",
    "X_val1_private = X_val1[task1_private_cols_lst]\n",
    "X_test1_private = X_test1[task1_private_cols_lst]\n",
    "\n",
    "X_train2_mtl_private = X_train2_mtl[task2_private_cols_lst]\n",
    "X_val2_mtl_private = X_val2_mtl[task2_private_cols_lst]\n",
    "X_test2_mtl_private = X_test2_mtl[task2_private_cols_lst]\n",
    "\n",
    "y_train1_private = y_train1\n",
    "y_val1_private = y_val1\n",
    "y_test1_private = y_test1\n",
    "\n",
    "y_train2_mtl_private = y_train2_mtl\n",
    "y_val2_private = y_val2_mtl\n",
    "y_test2_private = y_test2_mtl\n",
    "\n",
    "X_train_shared = pd.concat([X_train1[shared_cols_lst], X_train2_mtl[shared_cols_lst]])\n",
    "y_train_shared = pd.concat([y_train1, y_train2_mtl])\n",
    "\n",
    "tmp = pd.concat([X_train_shared, y_train_shared], axis = 1)\n",
    "tmp = shuffle(tmp)\n",
    "tmp = tmp.drop_duplicates()\n",
    "\n",
    "X_train_shared = tmp[shared_cols_lst]\n",
    "X_train_shared = X_train_shared[:len(X_train1_private)]\n",
    "y_train_shared = tmp['Diagnosis']\n",
    "y_train_shared = y_train_shared[:len(X_train1_private)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab61890",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# auc history clear\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Shared-private Multi-Task Learning (MTL)\n",
    "\n",
    "input1 = Input(shape= (len(X_train1.columns.tolist()),), name = 'input1')\n",
    "x1 = Dense(100, activation = 'sigmoid')(input1)\n",
    "x1 = Dense(50, activation = 'sigmoid')(x1)\n",
    "#x1 = Dense(25, activation = 'sigmoid')(x1)\n",
    "x1 = Model(inputs = input1, outputs = x1)\n",
    "\n",
    "input2 = Input(shape= (len(X_train2_mtl.columns.tolist()),), name = 'input2')\n",
    "x2 = Dense(100, activation = 'sigmoid')(input2)\n",
    "x2 = Dense(50, activation = 'sigmoid')(x2)\n",
    "#x2 = Dense(25, activation = 'sigmoid')(x2)\n",
    "x2 = Model(inputs = input2, outputs = x2)\n",
    "\n",
    "shared = Concatenate()([x1.output,x2.output])\n",
    "shared = Model(inputs = [input1,input2], outputs = shared)\n",
    "\n",
    "\n",
    "shared_private1 = Concatenate(name = 'shared_private1')([x1.output, shared.output])\n",
    "shared_private2 = Concatenate(name = 'shared_private2')([x2.output, shared.output])\n",
    "\n",
    "output1 = Dense(1, activation = \"sigmoid\", name = 'output1')(shared_private1)\n",
    "output2 = Dense(1, activation = \"sigmoid\", name = 'output2')(shared_private2)\n",
    "\n",
    "shared_private = Model(inputs = [x1.input, x2.input],  outputs = [output1, output2])\n",
    "\n",
    "shared_private.compile(loss = {'output1' : loss1, 'output2' : loss2}, optimizer = 'adam', metrics = tf.keras.metrics.AUC())\n",
    "shared_private.summary()\n",
    "\n",
    "shared_private_model = shared_private.fit([X_train1,X_train2_mtl], \n",
    "                 [y_train1,y_train2_mtl],\n",
    "                 epochs = 1200, batch_size = 256 , \n",
    "                 validation_data = ([X_val1,X_val2_mtl],\n",
    "                                    [y_val1,y_val2_mtl]), \n",
    "                 verbose = 2) \n",
    "\n",
    "\n",
    "y_pred = shared_private.predict([X_test1,X_test2_mtl])[0].ravel()\n",
    "\n",
    "y = y_test1.to_numpy()\n",
    " \n",
    "sp_fpr1, sp_tpr1, thresholds3 = roc_curve(y, y_pred, pos_label = 1)\n",
    "\n",
    "print(\"Shared-private MTL task1 auc :\", auc(sp_fpr1,sp_tpr1))\n",
    "\n",
    "y_pred = shared_private.predict([X_test1,X_test2_mtl])[1].ravel()\n",
    "\n",
    "y = y_test2_mtl.to_numpy()\n",
    " \n",
    "sp_fpr2, sp_tpr2, thresholds4 = roc_curve(y, y_pred, pos_label = 1)\n",
    "\n",
    "print(\"Shared-private MTL task2 auc :\", auc(sp_fpr2,sp_tpr2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed25df17",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "style.available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43231127",
   "metadata": {},
   "outputs": [],
   "source": [
    "style.use('ggplot')\n",
    "plt.title(\"Shared_private MTL task2 loss\")\n",
    "plt.plot(shared_private_model.history['val_output1_loss'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['val'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8305477",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"fully-shared MTL loss\")\n",
    "plt.plot(Shared_private_model.history['val_loss'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['val'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096fbee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Shared_private MTL task1 ROC curve\")\n",
    "plt.plot(sp_fpr1, sp_tpr1)\n",
    "plt.plot([0,1], [0,1], '--')\n",
    "plt.ylabel(\"sensitivity\")\n",
    "plt.xlabel(\"1-specificity\")\n",
    "plt.legend(['ROC'])\n",
    "round(auc(sp_fpr1,sp_tpr1),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3e0b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"fully-shared MTL task2 ROC curve\")\n",
    "plt.plot(sp_fpr2, sp_tpr2)\n",
    "plt.plot([0,1], [0,1], '--')\n",
    "plt.ylabel(\"sensitivity\")\n",
    "plt.xlabel(\"1-specificity\")\n",
    "plt.legend(['ROC'])\n",
    "round(auc(sp_fpr2,sp_tpr2),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96e7042",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shared.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad8f7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''shap.initjs()\n",
    "\n",
    "explainer_shap = shap.DeepExplainer(model = mlp, data = x_train)\n",
    "\n",
    "shap_values = explainer_shap.shap_values(X=X_train.values[:500], ranked_outputs = True)\n",
    "\n",
    "shap.force_plot(explainer.expected_value, shap_values[0,:], x_train.iloc[0,:])\n",
    "\n",
    "shap.summary_plot(shap_values, X, plot_type=\"bar\")\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d00789d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''mlp = MLPClassifier(hidden_layer_sizes = (100,), activation = 'logistic', \n",
    "                    solver = 'sgd', learning_rate = 'constant',\n",
    "                    learning_rate_init=0.00001, batch_size = 32, \n",
    "                    alpha= 0.0001, tol = 0.0000001, random_state=None,\n",
    "                    early_stopping = False, verbose = True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98852ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''mlp = MLPClassifier(hidden_layer_sizes = (150,), activation = 'logistic', \n",
    "                    solver = 'adam', max_iter = 500, batch_size = 32, verbose = True)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6f60bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# 오버샘플링\n",
    "\n",
    "rus = RandomUnderSampler(sampling_strategy = 'majority')\n",
    "ros = RandomOverSampler(sampling_strategy = 'minority')\n",
    "smote = SMOTE(k_neighbors = 5)\n",
    "ads = ADASYN(random_state = 10, n_neighbors = 5)\n",
    "bsmote = BorderlineSMOTE(random_state = 10, k_neighbors = 3, m_neighbors = 10)\n",
    "\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8328a5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''vu_01_train = pd.read_csv('/Users/harock96/Documents/project/연대 인턴/data/vu_01_train.csv')\n",
    "vu_01_val = pd.read_csv('/Users/harock96/Documents/project/연대 인턴/data/vu_01_valid.csv')\n",
    "vu_01_test = pd.read_csv('/Users/harock96/Documents/project/연대 인턴/data/vu_01_test.csv')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc14515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오버샘플링\n",
    "\n",
    "rus = RandomUnderSampler(sampling_strategy = 'majority')\n",
    "ros = RandomOverSampler(sampling_strategy = 'minority')\n",
    "smote = SMOTE(k_neighbors = 3)\n",
    "ads = ADASYN(random_state = 10, n_neighbors = 5)\n",
    "bsmote = BorderlineSMOTE(random_state = 42, k_neighbors = 3, m_neighbors = 10)\n",
    "\n",
    "#X_train1, y_train1 = smote.fit_resample(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60889df",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''X_train, X_val, X_test = vu_01_train[input_cols_lst], vu_01_val[input_cols_lst], vu_01_test[input_cols_lst]\n",
    "y_train, y_val, y_test = vu_01_train['Diagnosis'], vu_01_val['Diagnosis'], vu_01_test['Diagnosis']'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab80ad0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
